{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# i.4 â€” Multi-Label Tabular\n",
        "\n",
        "# --- Clean & install compatible versions ---\n",
        "!pip -q uninstall -y scikit-learn scikit-learn-intelex umap-learn >/dev/null\n",
        "!pip -q install -U pip setuptools wheel >/dev/null\n",
        "!pip -q install -U \"scikit-learn==1.6.1\" \"autogluon==1.4.0\" >/dev/null\n",
        "\n",
        "# --- Imports ---\n",
        "import numpy as np, pandas as pd, random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "# --- Minimal MultilabelPredictor (based on AutoGluon tutorial) ---\n",
        "class MultilabelPredictor:\n",
        "    \"\"\"Maintain one TabularPredictor per label; optionally condition on earlier labels.\"\"\"\n",
        "    def __init__(self, labels, consider_labels_correlation=True, **tp_kwargs):\n",
        "        if len(labels) < 2:\n",
        "            raise ValueError(\"Use TabularPredictor for a single label; this class is for MULTI-label.\")\n",
        "        self.labels = list(labels)\n",
        "        self.consider_labels_correlation = consider_labels_correlation\n",
        "        self.predictors = {\n",
        "            lab: TabularPredictor(label=lab, **tp_kwargs) for lab in self.labels\n",
        "        }\n",
        "\n",
        "    def fit(self, train_data, tuning_data=None, **fit_kwargs):\n",
        "        td_full = train_data.copy()\n",
        "        vd_full = tuning_data.copy() if tuning_data is not None else None\n",
        "        for i, lab in enumerate(self.labels):\n",
        "            # drop labels that come AFTER the current one (autoregressive)\n",
        "            drop_labs = self.labels[i+1:] if self.consider_labels_correlation else [l for l in self.labels if l != lab]\n",
        "            td = td_full.drop(columns=drop_labs, errors=\"ignore\")\n",
        "            vd = None if vd_full is None else vd_full.drop(columns=drop_labs, errors=\"ignore\")\n",
        "            print(f\"Fitting predictor for label: {lab}\")\n",
        "            self.predictors[lab].fit(train_data=td, tuning_data=vd, **fit_kwargs)\n",
        "        return self\n",
        "\n",
        "    def predict(self, data, **kwargs):\n",
        "        df = data.copy()\n",
        "        out = pd.DataFrame(index=df.index)\n",
        "        for i, lab in enumerate(self.labels):\n",
        "            # when correlated, feed previous predictions back in as features\n",
        "            if self.consider_labels_correlation and i > 0:\n",
        "                for prev in self.labels[:i]:\n",
        "                    df[prev] = out[prev]\n",
        "            out[lab] = self.predictors[lab].predict(df, **kwargs)\n",
        "        return out\n",
        "\n",
        "    def evaluate(self, data, **kwargs):\n",
        "        df = data.copy()\n",
        "        scores = {}\n",
        "        for i, lab in enumerate(self.labels):\n",
        "            if self.consider_labels_correlation and i > 0:\n",
        "                for prev in self.labels[:i]:\n",
        "                    df[prev] = self.predictors[prev].predict(df, **kwargs)\n",
        "            print(f\"Evaluating {lab} ...\")\n",
        "            scores[lab] = self.predictors[lab].evaluate(df, **kwargs)\n",
        "        return scores\n",
        "\n",
        "# --- Tiny synthetic multi-label dataset (numeric + categorical + text) ---\n",
        "rng = np.random.default_rng(42); random.seed(42)\n",
        "N = 1200\n",
        "breeds = [\"mix\",\"labrador\",\"siamese\",\"bulldog\",\"persian\"]\n",
        "colors = [\"black\",\"white\",\"brown\",\"gold\",\"tabby\",\"tri\"]\n",
        "pos_tokens = [\"friendly\",\"playful\",\"gentle\",\"trained\",\"housebroken\",\"social\"]\n",
        "neg_tokens = [\"timid\",\"special_needs\",\"anxious\",\"senior\",\"requires_experience\",\"shy\"]\n",
        "\n",
        "age = rng.integers(1, 180, size=N)\n",
        "fee = np.round(rng.uniform(0, 300, size=N), 2)\n",
        "breed = rng.choice(breeds, size=N)\n",
        "color = rng.choice(colors, size=N)\n",
        "photo_amt = rng.integers(0, 6, size=N)\n",
        "def make_desc():\n",
        "    k_pos = rng.integers(1, 3)\n",
        "    words = rng.choice(pos_tokens, size=k_pos, replace=False).tolist()\n",
        "    if rng.random() < 0.4:\n",
        "        words += [rng.choice(neg_tokens)]\n",
        "    return \" \".join(words)\n",
        "desc = [make_desc() for _ in range(N)]\n",
        "\n",
        "score_base = (\n",
        "    -0.02*age + 0.004*fee + 0.15*(photo_amt>=3).astype(float)\n",
        "    + 0.25*np.array([\"trained\" in s for s in desc], float)\n",
        "    - 0.20*np.array([\"requires_experience\" in s for s in desc], float)\n",
        ")\n",
        "is_playful       = (score_base + 0.10*np.array([\"playful\" in s for s in desc], float) > 0.15).astype(int)\n",
        "is_senior        = (age > 96).astype(int) | np.array([\"senior\" in s for s in desc], int)\n",
        "needs_experience = (score_base < -0.15).astype(int) | np.array([\"requires_experience\" in s for s in desc], int)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Age\": age, \"Fee\": fee, \"Breed\": breed, \"Color\": color,\n",
        "    \"Description\": desc, \"PhotoAmt\": photo_amt,\n",
        "    \"is_playful\": is_playful, \"is_senior\": is_senior, \"needs_experience\": needs_experience\n",
        "})\n",
        "\n",
        "labels = [\"is_playful\",\"is_senior\",\"needs_experience\"]\n",
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.2, random_state=123,\n",
        "    stratify=df[labels].astype(str).agg('|'.join, axis=1)\n",
        ")\n",
        "\n",
        "# --- Train multilabel (tiny budget; fast on CPU) ---\n",
        "ml = MultilabelPredictor(\n",
        "    labels=labels,\n",
        "    consider_labels_correlation=True,          # set False to train independent heads\n",
        "    problem_type=\"binary\",                     # each head is binary\n",
        "    eval_metric=\"f1\"                           # per-label F1\n",
        ").fit(\n",
        "    train_data=train_df,\n",
        "    presets=\"medium_quality_faster_train\",\n",
        "    time_limit=120\n",
        ")\n",
        "\n",
        "# --- Evaluate & save ---\n",
        "scores = ml.evaluate(test_df)\n",
        "print(\"\\nPer-label scores:\", scores)\n",
        "\n",
        "pred = ml.predict(test_df)\n",
        "pred.to_csv(\"multilabel_predictions.csv\", index=False)\n",
        "print(\"\\nSaved: multilabel_predictions.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY-GQSKdUvb0",
        "outputId": "48424fd5-eae0-403b-9cf9-02e43583b2af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping scikit-learn-intelex as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20251027_062612\"\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20251027_062612-001\"\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20251027_062612-002\"\n",
            "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       11.43 GB / 12.67 GB (90.2%)\n",
            "Disk Space Avail:   61.50 GB / 107.72 GB (57.1%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality_faster_train']\n",
            "Using hyperparameters preset: hyperparameters='default'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting predictor for label: is_playful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Beginning AutoGluon training ... Time limit = 120s\n",
            "AutoGluon will save models to \"/content/AutogluonModels/ag-20251027_062612\"\n",
            "Train Data Rows:    960\n",
            "Train Data Columns: 6\n",
            "Label Column:       is_playful\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11706.16 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.18 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 1 | ['Fee']\n",
            "\t\t('int', [])    : 2 | ['Age', 'PhotoAmt']\n",
            "\t\t('object', []) : 3 | ['Breed', 'Color', 'Description']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 3 | ['Breed', 'Color', 'Description']\n",
            "\t\t('float', [])    : 1 | ['Fee']\n",
            "\t\t('int', [])      : 2 | ['Age', 'PhotoAmt']\n",
            "\t0.1s = Fit runtime\n",
            "\t6 features in original data used to generate 6 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.13s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 768, Val Rows: 192\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 119.87s of the 119.87s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.2 GB\n",
            "\t0.9538\t = Validation score   (f1)\n",
            "\t12.99s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 106.84s of the 106.84s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.1 GB\n",
            "\t0.9552\t = Validation score   (f1)\n",
            "\t0.88s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ... Training model for up to 105.95s of the 105.95s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9394\t = Validation score   (f1)\n",
            "\t0.7s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ... Training model for up to 105.15s of the 105.15s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9254\t = Validation score   (f1)\n",
            "\t0.6s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 104.46s of the 104.46s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t0.9143\t = Validation score   (f1)\n",
            "\t3.47s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ... Training model for up to 100.98s of the 100.97s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9032\t = Validation score   (f1)\n",
            "\t0.62s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ... Training model for up to 100.25s of the 100.25s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9206\t = Validation score   (f1)\n",
            "\t0.64s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 99.48s of the 99.48s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.1 GB\n",
            "\t0.9143\t = Validation score   (f1)\n",
            "\t2.61s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 96.83s of the 96.83s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t0.9412\t = Validation score   (f1)\n",
            "\t0.87s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 95.94s of the 95.94s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.1 GB\n",
            "\t0.9565\t = Validation score   (f1)\n",
            "\t11.9s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 84.02s of the 84.02s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "\t0.9062\t = Validation score   (f1)\n",
            "\t1.21s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.87s of the 82.77s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM': 0.4, 'NeuralNetTorch': 0.4, 'RandomForestGini': 0.2}\n",
            "\t0.9855\t = Validation score   (f1)\n",
            "\t0.29s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 37.55s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2338.8 rows/s (192 batch size)\n",
            "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
            "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
            "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
            "\tBase Threshold: 0.500\t| val: 0.9855\n",
            "\tBest Threshold: 0.500\t| val: 0.9855\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels/ag-20251027_062612\")\n",
            "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       11.05 GB / 12.67 GB (87.2%)\n",
            "Disk Space Avail:   61.45 GB / 107.72 GB (57.0%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality_faster_train']\n",
            "Using hyperparameters preset: hyperparameters='default'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting predictor for label: is_senior\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Beginning AutoGluon training ... Time limit = 120s\n",
            "AutoGluon will save models to \"/content/AutogluonModels/ag-20251027_062612-001\"\n",
            "Train Data Rows:    960\n",
            "Train Data Columns: 7\n",
            "Label Column:       is_senior\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11293.01 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.19 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 1 | ['Fee']\n",
            "\t\t('int', [])    : 3 | ['Age', 'PhotoAmt', 'is_playful']\n",
            "\t\t('object', []) : 3 | ['Breed', 'Color', 'Description']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 3 | ['Breed', 'Color', 'Description']\n",
            "\t\t('float', [])     : 1 | ['Fee']\n",
            "\t\t('int', [])       : 2 | ['Age', 'PhotoAmt']\n",
            "\t\t('int', ['bool']) : 1 | ['is_playful']\n",
            "\t0.1s = Fit runtime\n",
            "\t7 features in original data used to generate 7 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.12s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 768, Val Rows: 192\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 119.88s of the 119.88s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "\t0.9514\t = Validation score   (f1)\n",
            "\t1.24s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 118.63s of the 118.63s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "\t0.9626\t = Validation score   (f1)\n",
            "\t1.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ... Training model for up to 117.60s of the 117.60s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9457\t = Validation score   (f1)\n",
            "\t0.56s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ... Training model for up to 116.96s of the 116.95s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9457\t = Validation score   (f1)\n",
            "\t0.57s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 116.29s of the 116.29s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t0.9789\t = Validation score   (f1)\n",
            "\t1.2s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ... Training model for up to 115.08s of the 115.08s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9412\t = Validation score   (f1)\n",
            "\t0.56s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ... Training model for up to 114.39s of the 114.39s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9412\t = Validation score   (f1)\n",
            "\t0.58s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 113.70s of the 113.69s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "\t0.9474\t = Validation score   (f1)\n",
            "\t1.05s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 112.63s of the 112.63s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t0.9519\t = Validation score   (f1)\n",
            "\t1.43s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 111.18s of the 111.18s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "\t0.9529\t = Validation score   (f1)\n",
            "\t1.9s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 109.26s of the 109.25s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "\t0.9514\t = Validation score   (f1)\n",
            "\t1.6s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.88s of the 107.64s of remaining time.\n",
            "\tEnsemble Weights: {'CatBoost': 1.0}\n",
            "\t0.9789\t = Validation score   (f1)\n",
            "\t0.35s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.75s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 40797.7 rows/s (192 batch size)\n",
            "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
            "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
            "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
            "\tBase Threshold: 0.500\t| val: 0.9789\n",
            "\tBest Threshold: 0.500\t| val: 0.9789\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels/ag-20251027_062612-001\")\n",
            "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       11.01 GB / 12.67 GB (86.9%)\n",
            "Disk Space Avail:   61.43 GB / 107.72 GB (57.0%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality_faster_train']\n",
            "Using hyperparameters preset: hyperparameters='default'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting predictor for label: needs_experience\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Beginning AutoGluon training ... Time limit = 120s\n",
            "AutoGluon will save models to \"/content/AutogluonModels/ag-20251027_062612-002\"\n",
            "Train Data Rows:    960\n",
            "Train Data Columns: 8\n",
            "Label Column:       needs_experience\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11275.06 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.20 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 1 | ['Fee']\n",
            "\t\t('int', [])    : 4 | ['Age', 'PhotoAmt', 'is_playful', 'is_senior']\n",
            "\t\t('object', []) : 3 | ['Breed', 'Color', 'Description']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 3 | ['Breed', 'Color', 'Description']\n",
            "\t\t('float', [])     : 1 | ['Fee']\n",
            "\t\t('int', [])       : 2 | ['Age', 'PhotoAmt']\n",
            "\t\t('int', ['bool']) : 2 | ['is_playful', 'is_senior']\n",
            "\t0.1s = Fit runtime\n",
            "\t8 features in original data used to generate 8 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.12s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 768, Val Rows: 192\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 119.88s of the 119.87s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "\t0.9632\t = Validation score   (f1)\n",
            "\t1.58s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 118.27s of the 118.27s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "\t0.9637\t = Validation score   (f1)\n",
            "\t1.27s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ... Training model for up to 116.99s of the 116.99s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9664\t = Validation score   (f1)\n",
            "\t0.71s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ... Training model for up to 116.20s of the 116.19s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.963\t = Validation score   (f1)\n",
            "\t0.59s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 115.52s of the 115.52s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t0.9635\t = Validation score   (f1)\n",
            "\t1.05s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ... Training model for up to 114.46s of the 114.46s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9605\t = Validation score   (f1)\n",
            "\t0.63s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ... Training model for up to 113.74s of the 113.74s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t0.9605\t = Validation score   (f1)\n",
            "\t0.61s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 113.04s of the 113.03s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.1 GB\n",
            "\t0.9662\t = Validation score   (f1)\n",
            "\t1.05s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 111.97s of the 111.97s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t0.9669\t = Validation score   (f1)\n",
            "\t0.8s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 111.14s of the 111.14s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.1 GB\n",
            "\t0.9732\t = Validation score   (f1)\n",
            "\t2.9s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 108.22s of the 108.22s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.1 GB\n",
            "\t0.9627\t = Validation score   (f1)\n",
            "\t1.42s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.88s of the 106.71s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetTorch': 0.5, 'RandomForestGini': 0.25, 'NeuralNetFastAI': 0.25}\n",
            "\t0.9799\t = Validation score   (f1)\n",
            "\t0.31s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.63s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2245.4 rows/s (192 batch size)\n",
            "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
            "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
            "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
            "\tBase Threshold: 0.500\t| val: 0.9799\n",
            "\tBest Threshold: 0.500\t| val: 0.9799\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels/ag-20251027_062612-002\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating is_playful ...\n",
            "Evaluating is_senior ...\n",
            "Evaluating needs_experience ...\n",
            "\n",
            "Per-label scores: {'is_playful': {'f1': 0.9285714285714286, 'accuracy': 0.975, 'balanced_accuracy': np.float64(0.940630797773655), 'mcc': np.float64(0.9149855276286857), 'roc_auc': np.float64(0.9980287569573283), 'precision': 0.975, 'recall': 0.8863636363636364}, 'is_senior': {'f1': 0.9789029535864979, 'accuracy': 0.9791666666666666, 'balanced_accuracy': np.float64(0.9793388429752066), 'mcc': np.float64(0.9591774224021128), 'roc_auc': np.float64(0.9947913049517328), 'precision': 1.0, 'recall': 0.9586776859504132}, 'needs_experience': {'f1': 0.9583333333333334, 'accuracy': 0.9333333333333333, 'balanced_accuracy': np.float64(0.8693371001917063), 'mcc': np.float64(0.7989954900119854), 'roc_auc': np.float64(0.9850670971647665), 'precision': 0.934010152284264, 'recall': 0.983957219251337}}\n",
            "\n",
            "Saved: multilabel_predictions.csv\n"
          ]
        }
      ]
    }
  ]
}